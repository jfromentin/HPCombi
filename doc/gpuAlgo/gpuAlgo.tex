%Compilation
%pdflatex -synctex=1 -interaction=nonstopmode gpuAlgo.tex && bibtex gpuAlgo.aux && pdflatex -synctex=1 -interaction=nonstopmode gpuAlgo.tex && pdflatex -synctex=1 -interaction=nonstopmode gpuAlgo.tex

\newcommand{\path}{./}
\input{\path/entete}	
\input{\path/commandesImages}
\input{\path/glossaire}

\begin{document}
%\input{\path/tabulation}
\pagenumbering{gobble}
\input{\path/pageDeGarde}
\pagestyle{empty}
\pagestyle{fancy}

\setlength{\headheight}{12pt}

%\lhead{\leftmark}
\rhead{Daniel Vanzo}
\lfoot{\textsf{LRI}}
\cfoot{\thepage}
\rfoot{\MONTHENG\ \YEAR}

%\maketitle
\tableofcontents
\newpage


%\chapter{HPCombi On GPU}
%\label{label}
\pagenumbering{arabic}
 
\section{Introduction}
Semigroups and monoids are algebraic structures consisting of a set and an associative binary operation.
Monoids have the additional property that they contain a neutral element for the latter binary operation.
Semigroups and monoids are studied notably in the theory of finite automata and formal language \cite{pin2010mathematical}.
Several libraries, like libsemigroups \cite{libsemigroups} or HPcombi\cite{hpcombi}, are available to manipulate such objects, 
they allow good performances using advanced algorithms and \gls{cpu} vector instructions.

This document describes an attempt to port some related routines to \gls{gpu}, in particular the ones involved in the exploration of transformation semigroups \cite{pin2010mathematical}.
\Cref{note} sets the problem of transformation semigroups exploration, \autoref{part:algo} gives a detailed description of the algorithms used on the \gls{gpu}.
In \autoref{part:results} profiling results are compared and shows that \gls{gpu} allow significant speedup for some routines,
finally, in \autoref{part:ccl} we conclude and note that the benefit offered by \gls{gpu} is highly dependent on "how much" of the global algorithm is ported to \gls{gpu}.


\section{Transformation semigroups exploration}
\label{note}
Transformations on [1,n] are stored as an array, for example with n=4:\\
The array $2|4|1|3$ represents the transformation that sends 1 on 2, 2 on 4, 3 on 1 and 4 on 3.
Let $G$ be a set of transformations called generators. We aim at finding properties about the set of transformations generated by the compositions of those generators.
The composition of two transformation $g_2\circ g_1$ can be seen as the shuffling of $g_2$ according to $g_1$ :
\begin{eqnarray*}
g_1 &=& 2|4|1|3\\
g_2 &=& 1|3|4|2\\
g_2\circ g_1 &=& 3|2|1|4
\end{eqnarray*}

The algorithm starts by defining an initial set of transformations $F_0$ only containing the identity transformation $Id = 1|2|3|...|n$.
Let's name $k$ the stage number.
At each stage of the algorithm all new transformations from this set $F_k$ are composed with all generators from G.
The transformations are considered as new if there are in $T_k=F_k \backslash F_{k-1}$.
%~ Let $F_k$ and $T_k$ be respectively the set of already found transformations and the set of transformations to test at the stage number $k$.
Initially $T_0 = F_0 = \{Id\}$\\
%~ Initially $F_0 = \{Id\}$ and $T_0 = G$\\
At stage number $k$, all element of $T_k$ are composed with all generators in $G$. 
The resulting transformations are then compared with transformations in $F_k$ and added, if absent, in $F_k$ to form $F_{k+1}$.

To each transformation in $F_k$ we associate a suit of generators of lengh $k$, that permitted its computation.
A word is defined as a suit of generators to compose with the identity transformation Id.
Transformation can be associated with several different words.
The first words from which the algorithm obtains a particular transformation is stored in a hash table.
The keys for the hash table are set to be structures containing a hash value and a word. 
This particular structure has been chosen over a dictionary data-structure for the following reason :
\begin{itemize}
\item The hash value allows fast distinction of keys when hash value are different whitout computing the transformations.
\item Storing the word in the key structure allows to compute transformations and reliably compare them.
\end{itemize}
Detail discussion about keys comparison is in \autoref{part:equality}.

All transformations composition occurring at stage $k$ can be computed in parallel, there are ($|G|\times|T_k|$) of them.
After composing the transformations, all hash value can be computed in parallel as well. 
Depending on the hash table implementation, keys can, or not, be inserted in parallel in the hash table.\\
The first implementation of the algorithm uses Google Sparsehash \cite{sparsehash} as the hash table, which only allows for sequential insertions. 
Further implementation could use a hash table on the \gls{gpu} as shown in \cite{wen2011gpu} which allow for parallel insertion.

\autoref{output} illustrates a transformation semigroup exploration for the test case Bihecke 5 (details about test cases are in \autoref{part:results}). 
There are 8 generators of lengh 120. At stage $1$, $T_k=G$ and $F_k=G\cup\{Id\}$. 
Among the $|T_1| \times |G|=8*8$ words tested in stage 2, 36 new words have been found and added in $F_2$, which means $64-36=28$ transformations where duplicates. 
The algorithm runs until there is no new transformation to test, meaning the previous stage did not permit to found any new transformation.

\begin{table}
\centering
\begin{tabular}{ >{\centering\arraybackslash}p{1cm} |>{\centering\arraybackslash}p{3cm} > {\centering\arraybackslash}p{3cm} }
Stage (k) & Number of transformation to test: $|T_k|$ & Number of transformations: $|F_k|$\\
\hline
%~ 0 & 8 & 1 \\
%~ 1 & 36 & 9 \\
%~ 2 & 126 & 45 \\
%~ 3 & 356 & 171 \\
%~ 4 & 860 & 527 \\
%~ 5 & 1764 & 1387 \\
%~ 6 & 3054 & 3151 \\
%~ 7 & 4594 & 6205 \\
%~ 8 & 5714 & 10799 \\
%~ 9 & 5778 & 16513 \\
%~ 10 & 4118 & 22291 \\
%~ 11 & 2678 & 26409 \\
%~ 12 & 1358 & 29087 \\
%~ 13 & 486 & 30445 \\
%~ 14 & 136 & 30931 \\
%~ 15 & 28 & 31067 \\
%~ 16 & 8 & 31095 \\
%~ 17 & 0 & 31103 \\
0 & 1 & 1 \\
1 & 8 & 9 \\
2 & 36 & 45 \\
3 & 126 & 171 \\
4 & 356 & 527 \\
5 & 860 & 1387 \\
6 & 1464 & 3151 \\
7 & 3054 & 6205 \\
8 & 4594 & 10799 \\
9 & 5714 & 16513 \\
10 & 5778 & 22291 \\
11 & 4118 & 26409 \\
12 & 2678 & 29087 \\
13 & 1258 & 30445 \\
14 & 486 & 30931 \\
15 & 136 & 31067 \\
16 & 25 & 31095 \\
17 & 8 & 31103 \\
18 & 0 & 31103 \\
\end{tabular}
\caption{Transformation semigroup exploration progression.}
\label{output}
\end{table} 


%To explore all possible transformation generated by a set of generator,
%Those shuffeling operation are done efficiently with x86 vector instructions for n up to 256.
%GPUs could be used to do those operation for n > 256.

\section{\glstext{gpu} Algorithms}
\label{part:algo}
\subsection{General algorithm}
The general transformation semigroups exploration algorithm is described in \autoref{algo:general}.

\begin{algorithm}
\caption{Transformation semigroups exploration}
\label{algo:general}
\begin{algorithmic}
\STATE $F_0 = \{Id\}$
\STATE $T_0 = \{Id\}$
\FOR{$0 < k < maxStage$}
\STATE - Compose all elements in $T_k$ with all elements in $G$, resulting in a set of transformations called TMP.
\STATE - Compute the hash values of all transformations in TMP.
\STATE - Search for duplicates within TMP and eliminate them from TMP. \emph{This step is not done in all implementations, see \autoref{part:results} for more details.}
\STATE - Try inserting transformations from TMP into $F_{k+1}$ and compute $T_{k+1}=F_{k+1} \backslash F_k$
\IF{$|T_{k+1}|=0$}
\STATE Stop
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Composition}
\label{part:compose}
Let's assume one thread should compose all generators contained in a word to the identity transformation, Id. 
Two loops are necessary, one to iterate over the generators in the word, the second to iterate over the transformations elements.
The loop order are intergengeable, let's compare both possibilities.
\autoref{algo:composebad} shows the algorithm for which the outer loop iterates over the generators and the inner loop iterates over the elements:
\begin{itemize}
\item A temporary array resultTMP must be allocated to store intermediate results.
\item Multiple loop over the elements are needed to :
\begin{enumerate}
 \item Initialize the result array to identity,
 \item Compose with one generator,
 \item Copy temporary results into the result array.
 \end{enumerate}
\item Access to the generators array is only done once per generator,
 \item Accesses to the generators component are contiguous,
 \item Parallelization of the outer loop require synchronization of all treads at each iteration.
\end{itemize}
\autoref{algo:composegood} shows the algorithm for which the outer loop iterates over the elements and the inner loop iterates over the generators:
\begin{itemize}
 \item No temporary array is needed, memory consumption is halved compared to \autoref{algo:composebad},
 \item Only one loop over the elements is needed,
 \item Multiple access to the same generator are done, as many as the lenght of the transformations,
 \item Accesses to the generators component are not contiguous,
 \item The outer loop can easily be parallelized without need for explicit synchronization.
\end{itemize}

\gls{gpu} threads can be synchronized in a \gls{cbloc} whereas threads in different \glspl{cbloc} can't be synchronized \cite{doccuda}. 
Indeed, there is no guarantee \glspl{cbloc} are executed in parallel, as a consequence, synchronization between \glspl{cbloc} could lead to dead blocks. 
Hence, a parallel implementation of \autoref{algo:composebad} would require transformations elements to be spread over a maximum of 1024 threads, the maximum number of threads in a \gls{cbloc}.
Whereas, a parallel implementation of \autoref{algo:composegood} could spread transformations elements over $1024 \times 2^31$ threads, 
the maximum number of thread in a \gls{cbloc} times the maximum number of \gls{cbloc} following the first dimension.

The later limitation on parallelism, the doubling of memory consumption and the need for three loops over the elements in \autoref{algo:composebad} 
made us choose the \autoref{algo:composegood} for the starting point of the parallel implementation. This leading to 
\autoref{algo:composepar}, a parallelized version for \gls{gpu} of \autoref{algo:composegood}. 
The outer loop is basically replaced by a "if" statement selecting threads that should compute the instructions. Each thread applies all compositions to one element.
To adapt granularity, one can mix \autoref{algo:composegood} and \autoref{algo:composepar} to assign several elements to each thread on which to compose all generators in a word.

As the results of several words are requested at one stage, more parallelism can be extracted. 
Indeed, each word's suit of compositions can be computed in parallel. 
Hence, two levels of parallelism can be exploited, parallelism over several words and parallelism over elements of the transformations. One should find a good balance between both:
\begin{itemize}
\item When few word's suit of compositions are to be computed, transformations elements should be spread over lots of threads to exploit parallelism,
\item When lots of word's suit of compositions are to be computed, transformations elements should be assigned to few threads for greater granularity.
\end{itemize}

In our implementation of the exploration of the transformation semigroups, parallelism is dynamically tuned during the execution according to the number of word's suit of compositions to be computed.



\begin{algorithm}
\caption{Composition: Outer loop on generators, inner loop on elements}
\label{algo:composebad}
\begin{algorithmic}
\FOR{$0 < elem < size$}
\STATE $result(elem) = elem$
\ENDFOR
\FOR{$0 < j < sizeWord$}
\STATE $gen = word(j)$
\FOR{$0 < elem < size$}
\STATE $index = gen(elem)$
\STATE $resultTMP(elem) = result(index)$
\ENDFOR
\STATE $index = newGen(index)$
\STATE $resultTMP(elem) = result(index)$
\FOR{$0 < elem < size$}
\STATE $result(elem) = resultTMP(elem)$
\ENDFOR
%\STATE Synchronise
\ENDFOR
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Composition: Outer loop on elements, inner loop on generators}
\label{algo:composegood}
\begin{algorithmic}
\FOR{$0 < elem < size$}
\STATE $index = elem$
\FOR{$0 < j < sizeWord$}
\STATE $gen = word(j)$
\STATE $index = gen(index)$
\ENDFOR
\STATE $index = newGen(index)$
\STATE $result(elem) = index$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Composition: Outer loop on elements, inner loop on generators with multiple threads}
\label{algo:composepar}
\begin{algorithmic}
\IF{$0 < threadId < size$}
\STATE $index = threadId$
\FOR{$0 < j < sizeWord$}
\STATE $gen = word(j)$
\STATE $index = gen(index)$
\ENDFOR
\STATE $index = newGen(index)$
\STATE $result(threadId) = index$
\ENDIF
\end{algorithmic}
\end{algorithm}


\subsection{Hashing}
\label{part:hash}
Let $t$ be a transformation and $P_{t}$ be the polynomial $\displaystyle\sum_{i=1}^{l} t(i)X^i$, where n is the lengh of the transformation. The hash function is defined as $hash(t) = P_{t}(prime)$, where $prime$ is a prime number.
The polynomial value is computed with the Ruffini-Horner method \cite{borwein2012polynomials}.
\begin{algorithm}
\caption{Hashing}
\label{algo:hash}
\begin{algorithmic}
\STATE $prime = PrimeNumber$
\STATE $result = prime \times trans(0)$
\FOR{$1 < i < size$}
\STATE $\switch result += trans(i)$
\STATE $\switch result *= prime$
\ENDFOR
\end{algorithmic}
\end{algorithm}
Parallelism is obtained by assigning one hash value computation per thread.
One could parallelize the computing of one hash value and think of tuning parallelism dynamically as done for the words' computation. 
This is not trivial because the computation of a hash value involves overflows. 
As a consequence, a direct parallelization of \autoref{algo:hash} would give different results depending on the number of threads used for a hash value computation. 
Dynamically adapting the number of thread computing a hash value during an execution would lead two false results. 
A more advanced algorithm is needed, assuring the exacts same overflows occurs regardless of the number of threads involved in the hash value computation. 
As the hash values computation is not a bottleneck in our examples, this as not be tested, each thread compute's a hash value regardless of the size of the transformations.


\subsection{Transformations equality}
\label{part:equality}
When inserting a new element in the hash table, tests for keys equality are preformed.
Remind that a key is a structure containing a hash value and a word.
To compare two keys, first the hash values are compared. If they are different, transformations are different.
If the hash values are identical the transformations have to be compared coefficient by coefficient to reliably discriminate them. 
The word contained in the key structure allows to compute both transformations and compare them coefficient by coefficient as shown in \autoref{algo:equal}, which is a sequential version of the algorithm.
In the \gls{gpu} version each thread computes a coefficient for both transformation according to \autoref{algo:composepar}. 
Then each thread compares the resulting coefficients and store the result in the $equal$ variable. 
The sum of each thread's $equal$ variable is then computed and compared to the size of the transformations.

The execution time of the equality test on \gls{gpu} is dominated by \gls{cpu}/\gls{gpu} data copy, kernel launching latency and \gls{cpu}/\gls{gpu} synchronization. 
This is demonstrated in \autoref{benchgpu}, which is a profiling of the execution of the Bihecke 5 and Renner A 7 test cases (details about test cases are in \autoref{part:results}).
Testing equality on \gls{cpu} avoids the overhead of copying data to the \gls{gpu}, kernel launching latency and \gls{cpu}/\gls{gpu} synchronization.
\autoref{equalcpugpu} compares the time to test for equality in different test cases, on the \gls{cpu} on one hand and on the \gls{gpu}, on the other hand. 
When transformation sizes are small (120, 720) equality testing is faster on \gls{cpu}, when transformation sizes are big (13327, 130922) it is faster on \gls{gpu}. 
The size where \glspl{gpu} start to be faster than \glspl{cpu} seems to be around 1000.

All runs in this section are executed on a \gls{e51650} as the \gls{cpu} and on a \gls{1080} as the \gls{gpu}.

\begin{table}
\centering
\begin{tabular}{ p{2cm} |>{\centering\arraybackslash}p{2.5cm} |> {\centering\arraybackslash}p{1.5cm} > {\centering\arraybackslash}p{3cm} > {\centering\arraybackslash}p{3cm} }
 Test case & Kernel computation (s) & Data Copy (s) & Kernel launching latency (s) & \glstext{cpu}/\glstext{gpu} synchronization (s) \\
\hline
Bihecke 5 & 1.03 & 2.80 & 0.96 & 1.29 \\
Renner A 7 & 36.21 & 10.51 & 3.86 & 81.79 \\
\end{tabular}
\caption{Profiling of the equality testing kernel on \glstext{gpu} for the Bihecke 5 and Renner A 7 test cases.}
\label{benchgpu}
\end{table} 

\begin{table}
\centering
\begin{tabular}{ p{3cm} |>{\centering\arraybackslash}p{2cm} |> {\centering\arraybackslash}p{2.8cm} > {\centering\arraybackslash}p{0.2cm} > {\centering\arraybackslash}p{2.8cm} }
 Test case & Size & Time on \glstext{cpu} (s) && Time on \glstext{gpu} (s) \\
\hline
%Bihecke 4 & 24 & 0.0007 &$<$& 0.044 \\
Bihecke 5 & 120 & 0.187 &$<$& 3.37 \\
Bihecke 6 (partial) & 720 & 38.3 &$<$& 219 \\
Renner A 6 & 13327 & 7.85 &$>$& 1.11 \\
Renner A 7 & 130922 & 1320 &$>$& 45.6 \\
\end{tabular}
\caption{Time comparison of the equality testing kernel on \glstext{gpu} and \glstext{cpu} for the Bihecke 5, Bihecke 6, Renner A 6 and Renner A 7 test cases.}
\label{equalcpugpu}
\end{table} 
 
\begin{algorithm}
\caption{Equality testing}
\label{algo:equal}
\begin{algorithmic}
\IF{$0 < threadId < size$}
\STATE $index = threadId$
\STATE equal = 0
\FOR{$0 < j < sizeWord$}
\STATE $gen1 = word(j)$
\STATE $gen2 = word(j)$
\STATE $index1 = gen1(index1)$
\STATE $index2 = gen2(index2)$
\ENDFOR
\IF{index1 = index2}
\STATE equal = 1
\ENDIF
\ENDIF
\STATE Sum(equal)
\end{algorithmic}
\end{algorithm}

\subsection{Duplicates' elimination}
\label{part:preinsert}

At each stage, new computed transformations are inserted in the hash table located on the \gls{cpu}. 
This require testing if the hash table location attributed to the transformation is empty, introducing a memory fetch. 
Those memory fetches are random as a consequence of the hash function efficiency. 
When the hash table becomes big enough, it doesn't fit entirely in the \gls{cpu} cache and lots of cache misses occurs.
To limit the number of cache misses we should limit the number of insertion attempts in the hash table. 
For that purpose a kernel is executed on the \gls{gpu} to eliminate duplicates within the set of transformations computed at a particular stage. 
This kernel is described in \autoref{algo:preinsert}. 
As a result, the set of transformations the \gls{cpu} attempts to insert in the hash table is smaller. 
\autoref{preinsert} shows that the number of transformations the \gls{cpu} attemps to insert is about 3 times smaller in typical test cases 
(details about test cases are in \autoref{part:results}).

In our implementation, hash value are computed before the elimination of duplicates. 
This allows the duplicates' elimination algorithm to first compare hash value before comparing the transformations coefficient by coefficient. 
The downside of this functions ordering is that some hash value computation could be avoided. 
Indeed, if the duplicates' elimination algorithm is run before computing the hash value, 3 times less hash value would be computed. 
Benchmarks in \autoref{part:results} show that the hash values computation is cheap compared to the elimination of duplicates, 
hence computing the hash value first to accelerate duplicates' elimination is the better choice.

As for the composition algorithm, a \gls{gpu} version of \autoref{algo:preinsert} is obtained by replacing the outer loop with a "if" statement 
selecting threads that should compute the instructions.

Note that the inner loop iterating over the elements of a transformation in \autoref{algo:preinsert}, could be stopped the first time a non equality occurs. 
This happened to be much slower for the test cases described in \autoref{part:results}. Two hypothesis, yet to be confirmed, could explain why :
\begin{itemize}
\item The additional loop control instructions resulting from the \emph{break} instruction are not efficiently handled by the \gls{gpu}.
\item The \emph{break} instruction results in more warp divergence, which implies serialization of execution \cite{doccuda}.
\end{itemize}

\begin{table}
\centering
\begin{tabular}{ p{3cm} |>{\centering\arraybackslash}p{2cm} > {\centering\arraybackslash}p{3cm} }
Test case & Size & \% of eliminated transformations \\
\hline
%Bihecke 4 & 24 & 57\% \\
Bihecke 5 & 120 & 64\% \\
Bihecke 6 (partial) & 720 & 69\% \\
Renner A 6 & 13327 & 64\% \\
Renner A 7 & 130922 & 66\% \\
\end{tabular}
\caption{Efficiency of the duplicates' elimination kernel for the Bihecke 5, Bihecke 6, Renner A 6 and Renner A 7 test cases.}
\label{preinsert}
\end{table} 

\begin{algorithm}
\caption{Eliminating duplicates}
\label{algo:preinsert}
\begin{algorithmic}
\FOR{$0 < trans < nb\_trans$}
\STATE $equal = 0$
\FOR{$0 < other < trans$}
\IF{hash(trans) = hash(other)}
\FOR{$0 < i < size$}
\IF{trans(i) = other(i)}
\STATE equal += 1
\ENDIF
\ENDFOR
\IF{equal = size}
\STATE Suppress $trans$
\ENDIF
\ENDIF
\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Profiling results}
\label{part:results}
In this section we aim at profiling the whole algorithm of enumerating all transformation generated by a set of generators. 
The algorithm parts described in \autoref{part:compose}, \autoref{part:hash}, \autoref{part:equality} and \autoref{part:preinsert} are profiled as well as the insertion time in the hash table.
Three implementations are compared:
\begin{enumerate}
\item The whole algorithm is executed on the \gls{cpu} (composition, hashing, equality testing routines and hash table) with non optimized code. No duplicate elimination is executed.
This is our reference implementation which we name "\glstext{cpu}". 
\item The composition, hashing and equality testing routinesare axecuted on \gls{gpu} and the hash table is located on the \gls{cpu}. No duplicate elimination is executed.
We name this implementation "\glstext{gpu}".
\item The composition, hashing and equality testing routinesare axecuted on \gls{gpu} and the hash table is located on the \gls{cpu}. Duplicate elimination is executed on the \gls{gpu}.
We name this implementation "\glstext{gpu}++".
\end{enumerate}
The four test cases are the following:
\begin{itemize}
\item Bihecke 5: 8 generators of lengh 120, generating a set of 31103 transformations,
\item Bihecke 6: 10 generators of lengh 720, generating a set of 7505009 transformations,
\item Renner A6: 6 generators of lengh 13327, generating a set of 13327 transformations,
\item Renner A7: 7 generators of lengh 130922, generating a set of 130922 transformations.
\end{itemize}


All runs in this section are executed on a \gls{e51650} as the \gls{cpu} and on a \gls{1080} as the \gls{gpu}.
The test case Bihecke 6 require running several weeks to complete, for this study runs where done only for a few hours.
As a consequence results about Bihecke 6 are partial: it is the results obtained at stage 12.
%For simplicity equality testing is always done on the \gls{gpu}, regardless of the discussion held in \autoref{part:equality}.


%The size of the transformations has a notable influence on performances. The high thread count of \glspl{gpu} allow to efficiently compute big transformations.

\Cref{bench:bihecke5,bench:bihecke6,bench:rena6,bench:rena7} show that a noticeable speed-up is obtained from the \gls{gpu} for the Composition and Hashing routines. 
Conclusion are to be lessened, first because the \gls{cpu} routines are not optimized, second because the overall time proportion of those two routines are low.\\
As discussed in \autoref{part:equality}, the equality testings are longer on the \gls{gpu} when transformations are rather small (\Cref{bench:bihecke5,bench:bihecke6}), 
faster on the \gls{gpu} when transformations are rather big (\Cref{bench:rena6,bench:rena7}).

For the Bihecke 5 and Bihecke 6 (\Cref{bench:bihecke5,bench:bihecke6}) test cases, the overhead of the duplicates' elimination routine 
is negligible compared to the total execution time ($<1\%$).
For the Renner A 6 and Renner A 7 (\Cref{bench:rena6,bench:rena7}) test cases, the overhead of the duplicates' elimination routine is low ($<10\%$) but not negligible.
In all test case the duplicates' elimination routine allow a speed-up over 3 for the hash table insertion time. 
This is because about $2/3$ of the transformations computed at each stage are eliminated as duplicates. Details are in \autoref{part:preinsert}.
\autoref{bench:rena7} shows a speed-up of 5 when using the duplicates' elimination routine because of an unexpectedly high insertion time for the \gls{cpu} implementation. 
This remain unexplained as for now.


\begin{table}
\centering
\begin{tabular}{ p{2.5cm} |>{\centering\arraybackslash}p{1.3cm} |> {\centering\arraybackslash}p{2cm} > {\centering\arraybackslash}p{1.3cm} > 
							{\centering\arraybackslash}p{1.3cm} > {\centering\arraybackslash}p{1.8cm} > {\centering\arraybackslash}p{1.3cm} }
 & Total time (s) & Composition (s) & Hash (s) & Equality (s) & duplicates' elimination (s) & Insert (s) \\
\hline
\glstext{cpu} & 66.9 & 0.02 & 0.03 & 0.55 & 0.00 & 66.2 \\

\glstext{gpu} & 72.6 & 0.002 & 0.002 & 9.3 & 0.00 & 63.3 \\

\glstext{gpu}++ & 27.2 & 0.002 & 0.002 & 3.37 & 0.06 & 23.8 \\
\hline
speed-up \newline \glstext{cpu}/\glstext{gpu}++ & 3 & 10 & 15 & 1/6 & 0 & 3 \\
\end{tabular}
\caption{Profiling of the test case Bihecke 5 for three implementations.}
\label{bench:bihecke5}
\end{table}


\begin{table}
\centering
\begin{tabular}{ p{2.5cm} |>{\centering\arraybackslash}p{1.3cm} |> {\centering\arraybackslash}p{2cm} > {\centering\arraybackslash}p{1.3cm} > 
							{\centering\arraybackslash}p{1.3cm} > {\centering\arraybackslash}p{1.8cm} > {\centering\arraybackslash}p{1.3cm} }
 & Total time ($10^4$ s) & Composition (s) & Hash (s) & Equality (s) & duplicates' elimination (s) & Insert ($10^4$ s) \\
\hline
\glstext{cpu} & 12.3 & 5.75 & 15.6 & 151 & 0.00 & 12.3 \\

\glstext{gpu} & 12.3 & 0.258 & 0.858 & 712 & 0.00 & 12.3 \\

\glstext{gpu}++ & 4.08 & 0.259 & 0.842 & 220 & 155 & 4.04 \\
\hline
speed-up \newline \glstext{cpu}/\glstext{gpu}++ & 3 & 22 & 15 & 2/3 & 0 & 3 \\
\end{tabular}
\caption{Profiling of the test case Bihecke 6 (partial) for three implementations.}
\label{bench:bihecke6}
\end{table}



\begin{table}
\centering
\begin{tabular}{ p{2.5cm} |>{\centering\arraybackslash}p{1.3cm} |> {\centering\arraybackslash}p{2cm} > {\centering\arraybackslash}p{1.3cm} > 
							{\centering\arraybackslash}p{1.3cm} > {\centering\arraybackslash}p{1.8cm} > {\centering\arraybackslash}p{1.3cm} }
 & Total time (s) & Composition (s) & Hash (s) & Equality (s) & duplicates' elimination (s) & Insert (s) \\
\hline
\glstext{cpu} & 46.3 & 1.22 & 2.12 & 32.6 & 0.00 & 9.88 \\

\glstext{gpu} & 12.6 & 0.068 & 0.066 & 3.24 & 0.00 & 9.16 \\

\glstext{gpu}++ & 4.64 & 0.062 & 0.065 & 1.09 & 0.381 & 3.03 \\
\hline
speed-up \newline \glstext{cpu}/\glstext{gpu}++ & 10 & 20 & 33 & 30 & 0 & 3 \\
\end{tabular}
\caption{Profiling of the test case Renner A6 for three implementations.}
\label{bench:rena6}
\end{table}




\begin{table}
\centering
\begin{tabular}{ p{2.5cm} |>{\centering\arraybackslash}p{1.3cm} |> {\centering\arraybackslash}p{2cm} > {\centering\arraybackslash}p{1.3cm} > 
							{\centering\arraybackslash}p{1.3cm} > {\centering\arraybackslash}p{1.8cm} > {\centering\arraybackslash}p{1.3cm} }
 & Total time (s) & Composition (s) & Hash (s) & Equality (s) & duplicates' elimination (s) & Insert (s) \\
\hline
\glstext{cpu} & 8440 & 260 & 238 & 5870 & 0.00 & 1990 \\

\glstext{gpu} & 1390 & 10.2 & 12.7 & 155 & 0.00 & 1170 \\

\glstext{gpu}++ & 524 & 10.2 & 12.7 & 45.9 & 22.1 & 391 \\
\hline
speed-up \newline \glstext{cpu}/\glstext{gpu}++ & 16 & 25 & 19 & 128 & 0 & 5 \\
\end{tabular}
\caption{Profiling of the test case Renner A7 for three implementations.}
\label{bench:rena7}
\end{table}




\section{Conclusions}
\label{part:ccl}

For all test cases, the composition and hashing routines benefit from the use of \gls{gpu}.
The speedups (from 10 to 33) should be interpreted with caution as the reference \gls{cpu} routines are not optimized.\\
The equality testing routine benefit greatly (speedups at most 128) from \gls{gpu} when the transformations are very big (13327, 130922).
Conversely, when the transformations are quit small (120, 720) the equality testing on \gls{gpu} is slower than on \gls{cpu}.

As the insertion time is dominant in the total execution time (due to cache misses, see \autoref{part:preinsert}), 
the duplicates' elimination on \gls{gpu} is quit valuable. 
For the Bihecke (5 and 6) test cases, the total speed-up is actually equal or very close to the insertion time speed-up: 3.
For the Renner (6 and 7) test cases, duplicates' elimination allows to noticeably reduce the equality checking time.
Note that similar speed-ups could not have been obtained on the \gls{cpu} with this hash table implementation (Google Sparsehash \cite{sparsehash}). 
Indeed, this hash table implementation only allows sequential insertion, furthermore, memory latency is a bottleneck even for sequential insertion.
It is the fast \gls{gpu} memory random accesses that permit those speed-ups.

The benefit from \gls{gpu} for some routine can be highly degraded by the \gls{cpu}/\gls{gpu} copy and synchronization time,
hence the more routines ported on \gls{gpu} the better performance we can get.
In our particular application random accesses to \gls{ram} memory is the bottleneck.
Further work could focus on addressing this bottleneck from two perspectives:
\begin{itemize}
\item Locating the hash table on the \gls{gpu} \cite{wen2011gpu},
\item Using another data structure like Tries \cite{briandaisfile}.
\end{itemize}




\section{Sources}
The source code is hosted on the rennergpu branch of HPCombi Github repository.
It can be cloned with this command:\\ 
\emph{git clone -b rennergpu https://github.com/hivert/HPCombi.git}

\input{\path/annexes}

\newpage
\bibliographystyle{plain}
\bibliography{biblio}

\end{document}

